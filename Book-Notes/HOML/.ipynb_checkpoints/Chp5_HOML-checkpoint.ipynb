{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Support Vector Machines\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classification\n",
    "SVMs are good tools for classification on small / medium sized datasets. The method focuses on creating a large margin between test samples from different groups. That is, form a decision boundary that is very far from the data while achieving accurate classification.\n",
    "\n",
    "The method is very sensative to scaling. \n",
    "\n",
    "Hard margin classification - when the data is linearlly seperable - is also very, very sensative to outliers.\n",
    "\n",
    "Soft margin classification balances keeping the margins large while tolerating a certain number of margin violations (i.e. letting data fall within the margins defined by the support vectors).\n",
    "\n",
    "Hyper-parameter $C$ in scikit - learn controlls this tradeoff. High values of $C$ allow few margin violations while low values of $C$ allow many. $C$ is also good to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, (2,3)]\n",
    "y = (iris['target'] == 2).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('linear_svc', LinearSVC(C=1, loss = 'hinge'))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5,1.7], [2.5,6.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use higher dimensional data using feature maps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples = 100, noise = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_svm_clf = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree = 3)), \n",
    "    ('scaler', StandardScaler()), \n",
    "    ('svm_clf', LinearSVC(C = 10, loss = 'hinge'))\n",
    "])\n",
    "\n",
    "poly_svm_clf.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the kernel trick super effectively within the SVM framework. We just pass the kernel to the LinearSVC constructor. The degree is the dimension of the kernel and _coef0_ is the decay in influence between low and high order terms in the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('svm_clf', SVC(kernel = 'poly', degree = 3, coef0 = 1, C = 5))\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create similarity scores using a RBF Kernel and expand dimensions this way. _gamma_ is the scaling paramter in the Gaussian kernel. $\\gamma$ controls how similar points can be. If set very small, all points are equally dissimilar, and the dimension expansion does very little. (Overfitting: reduce $\\gamma$. Underfitting: incrase $\\gamma$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, coef0=1, gamma=5))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('svm_clf', SVC(kernel = 'rbf', gamma = 5, coef0 = 1, C = 0.001))\n",
    "])\n",
    "\n",
    "rbf_kernel_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "\n",
    "\n",
    "LinearSVC - scales like $O(nm)$, take longer if higher precision required, does not support kernels. \n",
    "SVC - scales like $[O(m^2n), O(m^3n)]$, supports kernels\n",
    "\n",
    "This is why SVMs are not recommended with several data points - even if it scales well with the number of paramters. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "\n",
    "Reverse objective - fit as many points between the margins as possible, limiting the number of margin violations. \n",
    "\n",
    "The width of the margin is controlled by a hyper paramter $\\epsilon$. Any fit that does not change when $\\epsilon \\leq \\epsilon_0$ is said to be $\\epsilon_0$-insensative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR \n",
    "svm_reg = LinearSVR(epsilon = 1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel ='poly', degree= 3, C = 5, epsilon = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Details\n",
    "\n",
    "\n",
    "Linear SMV : $\\hat{y} = \\mathbf{1}(w^Tx + b \\geq 0)$\n",
    "\n",
    "The decision boundary is when $\\hat{y} = 0$ and we define the margins where $\\hat{y} = \\pm 1$. We look to change $(w,b)$ to maximize the distance between where $\\hat{y} = 0$ and $\\hat{y} = \\pm 1$. \n",
    "\n",
    "Note that slope of the decision function is $\\|w\\|$. And notice by reducing $\\|w\\|$ we _increase_ the distance between the margins. We also want to make little to no errors in classification while maintaining this large margin. Let $t_i = -1$ if $y_i = 0$ and $t_i = 1$ if $y_i = 1$. \n",
    "\n",
    "Then we can write the **hard margin** linear svm objective as \n",
    "\n",
    "$$(\\hat{w}, \\hat{b}) = \\min_{w,b} \\frac{1}{2}\\|w\\|^2 \\quad \\text{subject to}\\quad t_i(w^Tx_i + b)\\geq 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve soft margin, we need to allow for the constraint $t(w^Tx + b)$ to be allowed inside the margin. We do so by introducing a set of slack variables $\\{\\xi_i\\}_{i=1}^m$ and write the **soft margin** linear svm objective as \n",
    "\n",
    "$$(\\hat{w}, \\hat{b}) = \\min_{w,b} \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^m\\xi_i \\quad \\text{subject to}\\quad t_i(w^Tx_i + b)\\geq 1 - \\xi,\\quad \\xi_i \\geq 0$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these objectives can be solved using convex quadratic programming. For kernel problems you consider the dual of the QP. This dual relies only on inner products of the observations so we can use Mercer's theorem to avoid full dimension expansion. \n",
    "\n",
    "Mercers Theorem: Under suitable conditions, there exists a feature map $\\phi(\\cdot)$ such that $K(x,y) = \\phi(x)^T\\phi(y)$. \n",
    "\n",
    "Therefore, instead of applying the potentially infinite feature map $\\phi$ its enough to pass the kernel values which is at most $O(m^2)$ operations to the QP. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online versions of this problem instead use GD with $$J(w,b) = \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^m \\max(0, 1 - t_i (w^Tx_i + b))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
