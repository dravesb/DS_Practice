{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "from sklearn.tree import DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] \n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth = 2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize tree \n",
    "from sklearn.tree import export_graphviz \n",
    "export_graphviz(\n",
    "    tree_clf, \n",
    "    out_file = '/Users/benjamindraves/Desktop/DravesDocs/Book-Notes/HOML/figures/iris_tree.dot', \n",
    "    feature_names = iris.feature_names[2:], \n",
    "    class_names = iris.target_names, \n",
    "    filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: dot: command not found\r\n"
     ]
    }
   ],
   "source": [
    "#covert on command line to png \n",
    "!dot -Tpng iris_tree.dot -o isis_tree.png\n",
    "#needs a full installation of graphviz ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class probability estimates can be found by finding the proportion of training observations in each leaf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5,1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5,1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "Decision trees are fit using the Classification adn Regression Tree (CART) algorithm. The algorithm is greedy in approach and tries to spilt the training set at every leaf to maximize the child node's purity. Purity is measured by the Gini impurity for leaf $\\ell$ is given by $G_{\\ell} = 1 - \\sum_{k=1}^Kp_{\\ell,k}^2$ where $p_{\\ell,k} = \\frac{\\#\\text{instances in class $k$ in leaf $\\ell$}}{\\#\\text{instances in leaf } \\ell}$.\n",
    "\n",
    "At every level of the tree, returns a variable to split on - feature $k$ - and the splitting value $t_k$. CART solves the 2-D optimization problem $\\hat{k}, \\hat{t}_k = \\arg\\max_{k,t_k}J(k,t_k)$ for $$J(k,t_k) = \\frac{m_{\\text{left}}}{m}G_{\\text{left}} + \\frac{m_{\\text{right}}}{m}G_{\\text{right}}$$\n",
    "\n",
    "The algorithm terminates on some stopping condition based on samples in a leaf, maximum depth, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "\n",
    "Predictions: $O(\\log(m))$ where $m$ is the sample size.\n",
    "\n",
    "Fitting: $O(nm\\log(m))$ - $nm$ for the maximization of $J(k, t_k)$ and $\\log(m)$ for the average # nodes.\n",
    "\n",
    "Depending on the size of $m$ - sorting can be marginally sped up with pre-sorting the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity Measures\n",
    "\n",
    "By setting _criterion_ = 'entropy' you can instead use entropy as a impurity measure. Entropy is given by $$H_{\\ell} = - \\sum_{k=1}^Kp_{\\ell, k}\\log_2(p_{\\ell, k})$$ and can lead to more balanced trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Since there are very little assumptions made on the data - these models tend to overfit. Some approaches include: \n",
    "\n",
    "1. Tree properties: Maximum depth restriction ($\\textit{max_depth}$), minimum samples to split ($\\textit{min_samples_split}$), minimum samples in leaf ($\\textit{min_samples_leaf}$), maximum leaf nodes ($\\textit{max_leaf_nodes}$), and maximum number of features evaluated at each node ($\\textit{max_features}$)\n",
    "2. Pruning: After letting the decision tree fully construct, delete some leaf nodes. The most simple way to decide this to run a $\\chi^2$ test _within_ each leaf node. If you fail to reject - drop that leaf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We can also train a decision tree for a regression problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic training set + noise\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=1985)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth = 2, random_state = 1985)\n",
    "tree_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only central difference between classification / regerssion in decision trees is that we update the optimization to minimize weighted MSE instead of weighted impurity.\n",
    "$$J_{\\text{regression}}(k, t_k) = \\frac{m_{\\text{left}}}{m}\\text{MSE}_{\\text{left}} + \\frac{m_{\\text{right}}}{m}\\text{MSE}_{\\text{right}}$$\n",
    "where $\\text{MSE}_{\\text{node}} = \\sum_{i\\in\\text{node}}(y_i - \\bar{y}_{\\text{node}})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instability\n",
    "\n",
    "Decision tree can fluxuate a lot if the decision boundaries are not orthogonal. Using PCA maybe helpful here. \n",
    "\n",
    "Small changes in the training data will result in could dramitcally change the model (again - non parametric overfitting issues). A lof of these issues can be remedied by averaging over the decision trees with Random Forrests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
